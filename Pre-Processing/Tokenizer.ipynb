{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73fd1ceb",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    " It's not very difficult to understand how to do it, but it is worth knowing why we\n",
    "do it. The smallest unit to process in language processing task is a token. It is very much like\n",
    "a divide-and-conquer strategy, where we try to make sense of the smallest units at a\n",
    "granular level and add them up to understand the semantics of the sentence, paragraph,\n",
    "document, and the corpus (if any) by moving up the level of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7887c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Raj\n",
      "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import LineTokenizer, SpaceTokenizer, TweetTokenizer\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "# download the word pakage\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a3a33",
   "metadata": {},
   "source": [
    "# Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e92d9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line tokenizer output : \n",
      "\n",
      "My name is Maximus Decimus Meridius, commander of the Armies of the North,General of the Felix Legions and loyal servant to the true emperor,Marcus Aurelius. \n",
      "Father to a murdered son, husband to a murdered wife. \n",
      "And I will have my vengeance, in this life or the next.\n"
     ]
    }
   ],
   "source": [
    "lTokenizer = LineTokenizer()\n",
    "text = \"\"\"My name is Maximus Decimus Meridius, commander of the Armies of the North,General of the Felix Legions and loyal servant to the true emperor,Marcus Aurelius. \n",
    "         \\nFather to a murdered son, husband to a murdered wife. \n",
    "         \\nAnd I will have my vengeance, in this life or the next.\"\"\"\n",
    "print(\"Line tokenizer output : \\n\")\n",
    "for sentence in lTokenizer.tokenize(text):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edb2e6",
   "metadata": {},
   "source": [
    "As you can see, it has returned a list of three strings, meaning the given input has\n",
    "been divided in to three lines on the basis of where the newlines are.\n",
    "LineTokenizer simply divides the given input string into new lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e723a8",
   "metadata": {},
   "source": [
    "## Space Tokenizer\n",
    "\n",
    "As the name implies, it is supposed to divide on split on space characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68bbc55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space Tokenizer output: \n",
      "['By', '11', \"o'clock\", 'on', 'Sunday,', 'the', 'doctor', 'shall', 'open', 'the', 'dispensary.']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"By 11 o'clock on Sunday, the doctor shall open the dispensary.\"\n",
    "sTokenizer = SpaceTokenizer()\n",
    "print(\"Space Tokenizer output: \")\n",
    "print(sTokenizer.tokenize(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49125e85",
   "metadata": {},
   "source": [
    "## Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4181a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['By', '11', \"o'clock\", 'on', 'Sunday', ',', 'the', 'doctor', 'shall', 'open', 'the', 'dispensary', '.']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"By 11 o'clock on Sunday, the doctor shall open the dispensary.\"\n",
    "print(word_tokenize(raw_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d68f1c",
   "metadata": {},
   "source": [
    "As you can see, the difference between SpaceTokenizer and word_tokenize()\n",
    "is clearly visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95879162",
   "metadata": {},
   "source": [
    "## Tweet Tokenizer\n",
    "Now, on to the last one. There's a special TweetTokernizer that we can use\n",
    "when dealing with special case strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ed642d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3']\n"
     ]
    }
   ],
   "source": [
    "tTokenizer = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3\"\n",
    "print(tTokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d4364",
   "metadata": {},
   "source": [
    "Tweets contain special words, special characters, hashtags, and smileys that we want to keep intact. \n",
    "\n",
    "As we see, the Tokenizer kept the hashtag word intact and didn't break it; the\n",
    "smileys are also kept intact and are not lost. This is one special little class that can\n",
    "be used when the application demands it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93382f86",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c795ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8da569b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Maximus', 'Decimus', 'Meridius', ',', 'commander', 'of', 'the', 'Armies', 'of', 'the', 'North', ',', 'General', 'of', 'the', 'Felix', 'Legions', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'Marcus', 'Aurelius', '.', 'Father', 'to', 'a', 'murdered', 'son', ',', 'husband', 'to', 'a', 'murdered', 'wife', '.', 'And', 'I', 'will', 'have', 'my', 'vengeance', ',', 'in', 'this', 'life', 'or', 'the', 'next', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"My name is Maximus Decimus Meridius, commander of the Armies\n",
    "of the North, General of the Felix Legions and loyal servant to the\n",
    "true emperor, Marcus Aurelius. Father to a murdered son, husband to\n",
    "a murdered wife. And I will have my vengeance, in this life or the\n",
    "next.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7bf39d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'maximu', 'decimu', 'meridiu', ',', 'command', 'of', 'the', 'armi', 'of', 'the', 'north', ',', 'gener', 'of', 'the', 'felix', 'legion', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'marcu', 'aureliu', '.', 'father', 'to', 'a', 'murder', 'son', ',', 'husband', 'to', 'a', 'murder', 'wife', '.', 'and', 'i', 'will', 'have', 'my', 'vengeanc', ',', 'in', 'thi', 'life', 'or', 'the', 'next', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "pStems = [porter.stem(t) for t in tokens]\n",
    "print(pStems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16b12b",
   "metadata": {},
   "source": [
    "As you can see in the output, all the words have been rid of the trailing 's', 'es', 'e', 'ed', 'al', and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "376f673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'nam', 'is', 'maxim', 'decim', 'meridi', ',', 'command', 'of', 'the', 'army', 'of', 'the', 'nor', ',', 'gen', 'of', 'the', 'felix', 'leg', 'and', 'loy', 'serv', 'to', 'the', 'tru', 'emp', ',', 'marc', 'aureli', '.', 'fath', 'to', 'a', 'murd', 'son', ',', 'husband', 'to', 'a', 'murd', 'wif', '.', 'and', 'i', 'wil', 'hav', 'my', 'veng', ',', 'in', 'thi', 'lif', 'or', 'the', 'next', '.']\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "lStems = [lancaster.stem(t) for t in tokens]\n",
    "print(lStems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955a3a4",
   "metadata": {},
   "source": [
    "'us', 'e', 'th', 'eral', \"ered\", and many more! ending trailing is droped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7423f00",
   "metadata": {},
   "source": [
    "As we compare the output of both the stemmers, we see that lancaster is clearly the\n",
    "greedier one when dropping suffixes. It tries to remove as many characters from the end as\n",
    "possible, whereas porter is non-greedy and removes as little as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72742c06",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "WordNetLemmatizer removes affixes only if it can find the resulting word in the\n",
    "dictionary. This makes the process of lemmatization slower than Stemming. Also, it\n",
    "understands and treats capitalized words as special words; it doesnâ€™t do any processing for\n",
    "them and returns them as is. To work around this, you may want to convert your input\n",
    "string to lowercase and then run lemmatization on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f417c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9289390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Maximus', 'Decimus', 'Meridius', ',', 'commander', 'of', 'the', 'Armies', 'of', 'the', 'North', ',', 'General', 'of', 'the', 'Felix', 'Legions', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'Marcus', 'Aurelius', '.', 'Father', 'to', 'a', 'murdered', 'son', ',', 'husband', 'to', 'a', 'murdered', 'wife', '.', 'And', 'I', 'will', 'have', 'my', 'vengeance', ',', 'in', 'this', 'life', 'or', 'the', 'next', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"My name is Maximus Decimus Meridius, commander of the Armies\n",
    "of the North, General of the Felix Legions and loyal servant to the\n",
    "true emperor, Marcus Aurelius. Father to a murdered son, husband to\n",
    "a murdered wife. And I will have my vengeance, in this life or the\n",
    "next.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e0a62d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'maximu', 'decimu', 'meridiu', ',', 'command', 'of', 'the', 'armi', 'of', 'the', 'north', ',', 'gener', 'of', 'the', 'felix', 'legion', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'marcu', 'aureliu', '.', 'father', 'to', 'a', 'murder', 'son', ',', 'husband', 'to', 'a', 'murder', 'wife', '.', 'and', 'i', 'will', 'have', 'my', 'vengeanc', ',', 'in', 'thi', 'life', 'or', 'the', 'next', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(t) for t in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f3409aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Maximus', 'Decimus', 'Meridius', ',', 'commander', 'of', 'the', 'Armies', 'of', 'the', 'North', ',', 'General', 'of', 'the', 'Felix', 'Legions', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'Marcus', 'Aurelius', '.', 'Father', 'to', 'a', 'murdered', 'son', ',', 'husband', 'to', 'a', 'murdered', 'wife', '.', 'And', 'I', 'will', 'have', 'my', 'vengeance', ',', 'in', 'this', 'life', 'or', 'the', 'next', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99364212",
   "metadata": {},
   "source": [
    "As you see, it understands that for nouns it doesn't have to remove the trailing\n",
    "'s'. But for non-nouns, for example, legions and armies, it removes suffixes and\n",
    "also replaces them. However, what itâ€™s essentially doing is a dictionary match. We\n",
    "shall discuss the difference in the output section.\n",
    "\n",
    "\n",
    "As we compare the output of the stemmer and the lemmatizer, we see that the stemmer\n",
    "makes a lot of mistakes and the lemmatizer makes very few mistakes. However, it doesn't\n",
    "do anything with the word 'murdered', and that is an error. Yet, as an end product,\n",
    "lemmatizer does a far better job of getting us the base form than the stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e71cf",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47d3e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Raj\n",
      "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg , stopwords \n",
    "nltk.download('gutenberg')\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "377abd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'King', 'James', 'Bible', ']', 'The', ...]\n",
      "1010654\n"
     ]
    }
   ],
   "source": [
    "gb_words = gutenberg.words('bible-kjv.txt')\n",
    "print(gb_words)\n",
    "print(len(gb_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebc71d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_filtered = [word for word in gb_words if len(word) >=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04d3e1",
   "metadata": {},
   "source": [
    "In this step is we are iterating over the entire list of words from Gutenberg, discarding all\n",
    "the words/tokens whose length is two characters or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04a4d7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642004"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f49524c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Raj\n",
      "[nltk_data]     Patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# removing the stop words \n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = [w for w in words_filtered if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d82ec",
   "metadata": {},
   "source": [
    "The first line simply loads words from the stopwords corpus into the stopwords\n",
    "variable for the english language. The second line is where we are filtering out\n",
    "all stopwords from the filtered word list we had developed in the previous\n",
    "example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f507a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368614"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897046db",
   "metadata": {},
   "source": [
    "### Freq count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df7b291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdistPlain = nltk.FreqDist(words)\n",
    "fdist = nltk.FreqDist(gb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc0a1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are the most common 10 words in the bag\n",
      "[('shall', 9760), ('unto', 8940), ('LORD', 6651), ('thou', 4890), ('thy', 4450), ('God', 4115), ('said', 3995), ('thee', 3827), ('upon', 2730), ('man', 2721)]\n",
      "Following are the most common 10 words in the bag minus the stopwords\n",
      "[(',', 70509), ('the', 62103), (':', 43766), ('and', 38847), ('of', 34480), ('.', 26160), ('to', 13396), ('And', 12846), ('that', 12576), ('in', 12331)]\n"
     ]
    }
   ],
   "source": [
    "print('Following are the most common 10 words in the bag')\n",
    "print(fdistPlain.most_common(10))\n",
    "print('Following are the most common 10 words in the bag minus the stopwords')\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167eedb",
   "metadata": {},
   "source": [
    "If you look carefully at the output, the most common 10 words in the unprocessed or plain\n",
    "list of words won't make much sense. Whereas from the preprocessed bag of words, the\n",
    "most common 10 words such as god, lord, and man give us a quick understanding that we\n",
    "are dealing with a text related to faith or religion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee3d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
